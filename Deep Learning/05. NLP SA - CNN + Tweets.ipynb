{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ6zhycx-LJa"
      },
      "source": [
        "## 1. Importar las dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NON9giQ1_eZy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Un7jTbQNedR6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Utilizaremos el tokenizador de este módulo\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN8EilQG-cwa"
      },
      "source": [
        "---\n",
        "## 2. Preprocesado de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52CTV4_q-hpX"
      },
      "source": [
        "### Carga de Ficheros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X5Qkr_1Zfgi9"
      },
      "outputs": [],
      "source": [
        "# Headers\n",
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "\n",
        "train_data = pd.read_csv(\n",
        "    r'D:\\Python Scripts & Notebooks\\Jupyter Notebooks\\Artificial Intelligence\\Deep Learning\\Procesamiento del Lenguaje Natural Moderno en Python\\RNC\\train.csv',\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\")\n",
        "\n",
        "test_data = pd.read_csv(\n",
        "    r'D:\\Python Scripts & Notebooks\\Jupyter Notebooks\\Artificial Intelligence\\Deep Learning\\Procesamiento del Lenguaje Natural Moderno en Python\\RNC\\test.csv',\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "DAwqYIO259aZ",
        "outputId": "70fbd8ad-3331-4259-fd2c-d473b28d3bf7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment          id                          date     query  \\\n",
              "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
              "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
              "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
              "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
              "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
              "\n",
              "              user                                               text  \n",
              "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
              "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
              "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
              "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
              "4           Karoli  @nationwideclass no, it's not behaving at all....  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Visualizar el Dataframe\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Md2zOM-SPH"
      },
      "source": [
        "El conjunto de datos de testing tiene 3 etiquetas diferentes (una negativa, una positiva y una neutra), mientras que el conjunto de datos de entrenamiento tiene solo dos, por lo que no usaremos el archivo de testing y dividiremos el archivo de entrenamiento más tarde nosotros mismos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N-6BBPb3-OfY"
      },
      "outputs": [],
      "source": [
        "# Copia en profundidad\n",
        "data = train_data.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLWayJ-5-7nN"
      },
      "source": [
        "### Limpieza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Qa1v91RSkgz1"
      },
      "outputs": [],
      "source": [
        "# Dropeamos los predictores que no necesitamos\n",
        "data.drop([\"id\", \"date\", \"query\", \"user\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qU-7WW0m9O5j"
      },
      "outputs": [],
      "source": [
        "# Función para limpiar los tweets\n",
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    # Eliminamos la @ y su mención\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    # Eliminamos los links de las URLs\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    # Nos quedamos solamente con los caracteres\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
        "    # Eliminamos espacios en blanco adicionales\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    \n",
        "    return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vq-mIZNdAUjE"
      },
      "outputs": [],
      "source": [
        "# Limpiamos los tweets con la función en la columna text\n",
        "data_clean = [clean_tweet(tweet) for tweet in data.text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sentiment\n",
              "0    800000\n",
              "4    800000\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cantidad de datos agrupados para el target\n",
        "data.groupby('sentiment').size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TqtCJZkhAb9C"
      },
      "outputs": [],
      "source": [
        "# Cambiamos las etiquetas del 4 al 1\n",
        "data_labels = data.sentiment.values\n",
        "data_labels[data_labels == 4] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTKZ5fUh_Kxz"
      },
      "source": [
        "### Tokenización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "IvmIKnAnAJRY",
        "outputId": "28ab3fe1-6a4d-460f-996f-441ec8ee2f57"
      },
      "outputs": [],
      "source": [
        "# Tokenizador\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(data_clean, target_vocab_size=2**16)\n",
        "\n",
        "# Entradas\n",
        "data_inputs = [tokenizer.encode(sentence) for sentence in data_clean]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysb2uib8n6b3"
      },
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "M9qttbt7BMwg"
      },
      "outputs": [],
      "source": [
        "# Obtenemos el valor máximo entre todas las palabras de las entradas\n",
        "max_len = max([len(sentence) for sentence in data_inputs])\n",
        "\n",
        "# Rellenamos con '0' la longitud de aquellos valores tokenizados hasta alcanzar al 'max_len'\n",
        "data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,\n",
        "                                                            value=0,\n",
        "                                                            padding=\"post\",\n",
        "                                                            maxlen=max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4Ac7EXNNblp"
      },
      "source": [
        "### Dividimos en los conjuntos de training y testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "x_H7zROsNbCE"
      },
      "outputs": [],
      "source": [
        "test_idx = np.random.randint(0, 800000, 8000)\n",
        "test_idx = np.concatenate((test_idx, test_idx+800000))\n",
        "\n",
        "# Test\n",
        "test_inputs = data_inputs[test_idx]\n",
        "test_labels = data_labels[test_idx]\n",
        "\n",
        "# Train\n",
        "train_inputs = np.delete(data_inputs, test_idx, axis=0)\n",
        "train_labels = np.delete(data_labels, test_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWu6hLDG_UJZ"
      },
      "source": [
        "---\n",
        "## 3. Construcción del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Construir y Compilar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import GlobalAveragePooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import adam_v2\n",
        "\n",
        "# Convolutional Neural Network\n",
        "def CNN():\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=tokenizer.vocab_size, output_dim=200))\n",
        "    model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu'))\n",
        "    model.add(Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu'))\n",
        "    model.add(Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu'))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Optimizer\n",
        "    adam = adam_v2.Adam(learning_rate=0.0001)\n",
        "    \n",
        "    # Compile\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                 optimizer=adam,\n",
        "                 metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92XbAZ9E1AMS"
      },
      "source": [
        "---\n",
        "## 4. Ajuste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining callback and compiling the model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "filepath = './checkpoints-weights.hdf5'\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, verbose=1)\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', mode='max', save_best_only=True, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instanciamos\n",
        "# model = CNN()\n",
        "\n",
        "# Reentrenar\n",
        "# model.fit(train_inputs,\n",
        "#           train_labels,\n",
        "#           batch_size=32,\n",
        "#           epochs=5,\n",
        "#           callbacks = [reduce_lr, early_stopping, checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cargar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar los pesos\n",
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('model_final.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Gn6JhJKXDK"
      },
      "source": [
        "### Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "Jt2dRZWhKHbT",
        "outputId": "aea243cd-e6d9-4124-9138-aa579ea12081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 4s 5ms/step - loss: 0.3640 - accuracy: 0.8384\n",
            "Accuracy: 83.84%\n"
          ]
        }
      ],
      "source": [
        "# Evaluar resultados con el conjunto de test\n",
        "results = model.evaluate(test_inputs, test_labels, batch_size=32)\n",
        "print(f'Accuracy: {round(results[1]*100, 2)}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RNC para NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
